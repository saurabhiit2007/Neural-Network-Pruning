Neural-Network-Pruning

Description
=============
Neural Network Pruning is a method of compressing single hidden layer neural network to reduce the model size. This machine learning algorithm has been specifically developed for resource constrained enviornments like IoT devices and other Edge devices. The pruning method is based on removing the neural connections from the network by making the corresponding weights as zero. The strategy has been derived from the following paper : "Learning both Weights and Connections for Efficient Neural Networks", Song Han, Jeff Pool, John Tran, William J. Dally. NIPS, 2015.


Usage
=============

Source Files : The repository contains five Matlab(.m) files for Neural Network pruning. The NeuralNetSparse.m, train.m and predict.m are the main source files. The nnCostFunction.m and fmincg.m are the supporting files that helps in training the network using Gradient Descent.

Description of source files:

1) The "NeuralNetSparse.m" is the main file that contains all the hyper-parameters, dataset and training parameters that are required to train the network. The different hyper-parameters help user to train Neuralnets with different configuration that helps in optimizing the accuracy v/s model size trade-off. Given a memory budget constraint depending upon the limitations of the underlying devices, there is a trade-off between the fat-sparse network and slim-dense network. This trade-off depends on the dataset used, learning rate and memory budget. So, in order to find out the best configuration NeuralNetSparse.m contains several parameters that can be tuned by doing a grid-search.

   Input to the Network:
    - dataset_name -> name of the dataset_name
    - num_labels -> # of output classes
    
    NOTE : Each row of the dataset conatins a data point. The first column of dataset contains the output labels starting from            1 and going upto # of classes. Rest of the columns contains the features. The dataset should be comma separated.
           
           Line 1: <Label of Point 1>,<Value1>, ... ,<ValueD> (without <>)
           Line 2: <Label of Point 2>,<Value1>, ... ,<ValueD> (without <>)
   
    - min_hidden_units -> minimum # of nodes to start with
    - jumpsize_hidden_units -> # of nodes to increase in each run
    - max_hidden_units -> maximum # of nodes in the network
    - lambdaLR -> Learning rate for the gradient descent
    - pruning_percent -> %age of weights to be pruned.
    - num_iter -> # of iterations for gradient descent
    
   Output of the Network
    - theta1 -> weight matrix from input layer to hidden layer (dim : [num_hidden_units, num_features])
    - theta2 -> weight matrix from hidden layer to output layer (dim : [num_classes, num_hidden_units])
    - model_size -> model size of the sparse network in CSR Format. The non-zero value is stored as 32-bit floating point,  			while the row and column indices are stored as 16-bit integer values.

   Files generated by the network
    - Output File -> Contains the train and test accuracy along with model size for given set of hyper-parameters
    - Model Files -> Contains sparse neural network model in CSR format. 
                     
		     Line 1: <total # of non-zero entries in theta1, total # of non-zero entries in theta2> (without <>)
		     Line 2: <row indices for theta1> (without <>)
		     Line 3: <non_zero values of theta1 in comma separated format> (without <>)
		     Line 4: <column indices non_zero values of theta1 in comma separated format> (without <>)
		     Line 5: <row indices for theta2> (without <>)
		     Line 6: <non_zero values of theta2 in comma separated format> (without <>)
		     Line 7: <column indices non_zero values of theta2 in comma separated format> (without <>)
  
2) The "train.m" contains the code for training, pruning and retraining the network using gradient descent. 
    - Training Step: The network is trained with the help of Matlab's in-built GD optimisation function fmincg.m. The                              fmincg.m uses the nnCostFunction.m that contains the implementation of forward & backward pass of the                          neural network. 
    - Pruning Step: After training, the weights are pruned based on their relative importance which is determenied by their                       absolute magnitude. The pruning percent depends on the memory budget given by the user. 
    - Retraining Step: Aggressive pruning results in significant drop in accuracy. In order to recover the lost accuracy, the                        network is retrained while keeping the mask of the zero-weights constant. After each gradient step the                        mask is applied on the weight matrix to remove the effect of updated zero-weights. 
    
3) The "predict.m" contains the code for prediction on the validation/test dataset. 
    

Running the code  
===================
In order to run the code, follow these steps :
 - Put the dataset files in dataset folder of the repo. The dataset files should have .train and .test as the extension. 
 - Change the dataset name in the "NeuralNetSparse.m" file 
 - For carrying out extensive grid search over different parameters, change the hyper-parameters in the "NeuralNetSparse.m"      file. 
 - Run "NeuralNetSparse.m" file. After completion of the code, the model files are saved in "Models" folder and output is        saved in "Output" folder. Both these folders are generated by the code itself.  
  
	
  
